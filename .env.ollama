# ============================================
# Ollama Configuration for TestCodeAgent
# ============================================
# Windows + uv 환경에서 Ollama 사용 설정
#
# 사용법:
# 1. 이 파일을 .env로 복사: copy .env.ollama .env
# 2. Ollama 설치: https://ollama.ai
# 3. 모델 다운로드: ollama pull deepseek-r1:14b
# 4. Ollama 서버 시작: ollama serve
# 5. Backend 실행: cd backend && uv run uvicorn app.main:app --reload

# =========================
# LLM Configuration (Ollama)
# =========================

# Ollama는 OpenAI 호환 API를 제공 (포트 11434)
LLM_ENDPOINT=http://localhost:11434/v1
LLM_MODEL=deepseek-r1:14b

# DeepSeek 모델이므로 model_type을 deepseek로 설정
MODEL_TYPE=deepseek

# Ollama는 단일 모델이므로 같은 모델 사용
VLLM_REASONING_ENDPOINT=http://localhost:11434/v1
VLLM_CODING_ENDPOINT=http://localhost:11434/v1
REASONING_MODEL=deepseek-r1:14b
CODING_MODEL=deepseek-r1:14b

# =========================
# Agent Framework Selection
# =========================
AGENT_FRAMEWORK=microsoft

# =========================
# API Server Configuration
# =========================
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# =========================
# CORS Configuration
# =========================
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# =========================
# Logging
# =========================
LOG_LEVEL=INFO
