"""Enhanced LangGraph Workflow - Production Implementation

This workflow implements:
- Supervisor ‚Üí Architect ‚Üí Coders ‚Üí Quality Gates ‚Üí HITL ‚Üí Persistence
- Real-time streaming of code generation
- Parallel execution where applicable
- Proper HITL checkpoints with manager registration
- Agent execution time tracking
- ETA estimation

CRITICAL: This workflow performs REAL operations.
"""

import asyncio
import logging
import time
from typing import AsyncGenerator, Dict, List, Any, Optional
from datetime import datetime
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver

from app.agent.langgraph.schemas.state import QualityGateState, create_initial_state, DebugLog

# Import nodes
from app.agent.langgraph.nodes.architect import architect_node
from app.agent.langgraph.nodes.coder import coder_node
from app.agent.langgraph.nodes.reviewer import reviewer_node
from app.agent.langgraph.nodes.refiner import refiner_node
from app.agent.langgraph.nodes.security_gate import security_gate_node
from app.agent.langgraph.nodes.qa_gate import qa_gate_node
from app.agent.langgraph.nodes.aggregator import quality_aggregator_node
from app.agent.langgraph.nodes.persistence import persistence_node

# Import Supervisor
from core.supervisor import SupervisorAgent

# Import HITL
from app.hitl import HITLManager, get_hitl_manager
from app.hitl.models import HITLCheckpointType, HITLRequest as HITLRequestModel, HITLStatus

logger = logging.getLogger(__name__)


# Agent display names and descriptions
AGENT_INFO = {
    "supervisor": {
        "title": "üß† Supervisor",
        "description": "Task Analysis & Planning",
        "icon": "brain"
    },
    "architect": {
        "title": "üèóÔ∏è Architect",
        "description": "Project Structure Design",
        "icon": "building"
    },
    "coder": {
        "title": "üíª Coder",
        "description": "Code Implementation",
        "icon": "code"
    },
    "reviewer": {
        "title": "üëÄ Reviewer",
        "description": "Code Quality Review",
        "icon": "eye"
    },
    "qa_gate": {
        "title": "üß™ QA Tester",
        "description": "Test Generation & Execution",
        "icon": "flask"
    },
    "security_gate": {
        "title": "üîí Security",
        "description": "Security Analysis",
        "icon": "shield"
    },
    "refiner": {
        "title": "üîß Refiner",
        "description": "Code Refinement",
        "icon": "wrench"
    },
    "aggregator": {
        "title": "üìä Aggregator",
        "description": "Results Aggregation",
        "icon": "chart"
    },
    "hitl": {
        "title": "üë§ Human Review",
        "description": "Awaiting Your Approval",
        "icon": "user"
    },
    "persistence": {
        "title": "üíæ Persistence",
        "description": "Saving Files",
        "icon": "save"
    }
}


class EnhancedWorkflow:
    """Enhanced production workflow with:
    - Architect Agent for project design
    - Parallel execution support
    - Real-time streaming
    - HITL checkpoints with proper registration
    - Execution time tracking
    """

    def __init__(self):
        """Initialize enhanced workflow"""
        self.supervisor = SupervisorAgent(use_api=True)
        self.hitl_manager = get_hitl_manager()
        self.memory = MemorySaver()
        logger.info("‚úÖ EnhancedWorkflow initialized")

    def _estimate_total_time(self, complexity: str, num_files: int) -> float:
        """Estimate total execution time based on complexity"""
        base_times = {
            "simple": 30,
            "moderate": 60,
            "complex": 120,
            "critical": 180,
        }
        base = base_times.get(complexity, 60)
        return base + (num_files * 5)

    def _get_agent_info(self, agent_name: str) -> Dict[str, str]:
        """Get display info for an agent"""
        return AGENT_INFO.get(agent_name, {
            "title": f"ü§ñ {agent_name.title()}",
            "description": "Processing...",
            "icon": "robot"
        })

    def _create_hitl_request(
        self,
        request_id: str,
        workflow_id: str,
        stage_id: str,
        checkpoint_type: HITLCheckpointType,
        title: str,
        description: str,
        content: Any,
        priority: str = "normal",
        allow_skip: bool = False
    ) -> HITLRequestModel:
        """Create and register HITL request with manager"""
        request = HITLRequestModel(
            request_id=request_id,
            workflow_id=workflow_id,
            stage_id=stage_id,
            checkpoint_type=checkpoint_type,
            title=title,
            description=description,
            content=content,
            priority=priority,
            allow_skip=allow_skip,
            status=HITLStatus.PENDING,
            created_at=datetime.utcnow()
        )

        # Register with manager
        self.hitl_manager._pending_requests[request_id] = request
        self.hitl_manager._workflow_requests[workflow_id].add(request_id)

        logger.info(f"[HITL] Request registered: {request_id}")
        return request

    async def execute(
        self,
        user_request: str,
        workspace_root: str,
        task_type: str = "general",
        enable_debug: bool = True
    ) -> AsyncGenerator[Dict, None]:
        """Execute enhanced workflow with streaming

        Yields real-time updates including:
        - Agent progress and status
        - Streaming code generation content
        - HITL checkpoints
        - Execution times
        """
        workflow_id = f"workflow_{datetime.utcnow().timestamp()}"
        start_time = time.time()
        agent_times: Dict[str, float] = {}
        completed_agents: List[str] = []

        logger.info(f"üöÄ Starting Enhanced Workflow: {workflow_id}")

        try:
            # ==================== PHASE 1: SUPERVISOR ====================
            yield self._create_update("supervisor", "starting", {
                "message": "Analyzing your request...",
                "workflow_id": workflow_id,
            })

            supervisor_start = time.time()
            supervisor_analysis = None
            thinking_blocks = []

            # Stream supervisor thinking
            async for update in self.supervisor.analyze_request_async(user_request):
                if update["type"] == "thinking":
                    thinking_blocks.append(update["content"])
                    yield self._create_update("supervisor", "thinking", {
                        "current_thinking": update["content"][:200] + "..." if len(update["content"]) > 200 else update["content"],
                        "thinking_stream": [t[:100] for t in thinking_blocks[-3:]],  # Last 3 blocks, truncated
                    })
                elif update["type"] == "analysis":
                    supervisor_analysis = update["content"]

            if not supervisor_analysis:
                supervisor_analysis = self.supervisor.analyze_request(user_request)

            agent_times["supervisor"] = time.time() - supervisor_start
            completed_agents.append("supervisor")

            estimated_total = self._estimate_total_time(
                supervisor_analysis.get("complexity", "moderate"), 10
            )

            yield self._create_update("supervisor", "completed", {
                "supervisor_analysis": supervisor_analysis,
                "task_complexity": supervisor_analysis.get("complexity"),
                "workflow_strategy": supervisor_analysis.get("workflow_strategy"),
                "execution_time": agent_times["supervisor"],
                "estimated_total_time": estimated_total,
                "completed_agents": completed_agents.copy(),
                "streaming_content": f"Task Analysis Complete\n‚Ä¢ Complexity: {supervisor_analysis.get('complexity')}\n‚Ä¢ Strategy: {supervisor_analysis.get('workflow_strategy')}",
            })

            # ==================== PHASE 2: ARCHITECT ====================
            yield self._create_update("architect", "starting", {
                "message": "Designing project architecture...",
            })

            architect_start = time.time()
            state = create_initial_state(
                user_request=user_request,
                workspace_root=workspace_root,
                task_type=supervisor_analysis.get("task_type", "implementation"),
                enable_debug=enable_debug
            )
            state["supervisor_analysis"] = supervisor_analysis

            architect_result = architect_node(state)
            agent_times["architect"] = time.time() - architect_start
            completed_agents.append("architect")

            architecture = architect_result.get("architecture_design", {})
            files_to_create = architect_result.get("files_to_create", [])

            # Generate project name from user request analysis
            project_name = self._generate_project_name(
                user_request=user_request,
                supervisor_analysis=supervisor_analysis,
                architecture=architecture
            )

            # Sanitize project name (remove special characters, lowercase, replace spaces)
            import re
            project_name = re.sub(r'[^\w\-]', '-', project_name.lower()).strip('-')
            # Remove consecutive dashes
            project_name = re.sub(r'-+', '-', project_name)
            if not project_name or len(project_name) < 2:
                project_name = f"project_{int(time.time())}"

            # Create project directory within workspace
            import os
            project_dir = os.path.join(workspace_root, project_name)
            try:
                os.makedirs(project_dir, exist_ok=True)
                logger.info(f"üìÅ Created project directory: {project_dir}")
            except Exception as e:
                logger.error(f"Failed to create project directory: {e}")
                project_dir = workspace_root  # Fallback to workspace root

            # Update workspace_root to project directory for subsequent nodes
            state["workspace_root"] = project_dir
            original_workspace = workspace_root
            workspace_root = project_dir

            # Send project info to frontend
            yield self._create_update("workflow", "project_info", {
                "project_name": project_name,
                "project_dir": project_dir,
                "original_workspace": original_workspace,
                "message": f"Created project: {project_name}",
                "streaming_content": f"üìÅ Project: {project_name}\nüìÇ Location: {project_dir}",
            })

            # Stream architecture design summary
            arch_summary = f"Project: {architecture.get('project_name', 'project')}\n"
            arch_summary += f"Tech Stack: {architecture.get('tech_stack', {}).get('language', 'python')}\n"
            arch_summary += f"Files to create: {len(files_to_create)}\n"
            for f in files_to_create[:3]:
                arch_summary += f"  ‚Ä¢ {f.get('path', 'unknown')}\n"
            if len(files_to_create) > 3:
                arch_summary += f"  ... and {len(files_to_create) - 3} more"

            yield self._create_update("architect", "completed", {
                "architecture_design": architecture,
                "files_to_create": files_to_create,
                "execution_time": agent_times["architect"],
                "completed_agents": completed_agents.copy(),
                "streaming_content": arch_summary,
            })

            # ==================== PHASE 3: CODING ====================
            yield self._create_update("coder", "starting", {
                "message": f"Generating {len(files_to_create)} files in parallel...",
                "files_count": len(files_to_create),
            })

            coder_start = time.time()
            state.update(architect_result)

            # PARALLEL STREAMING: Show multiple files being generated simultaneously
            # Group files into parallel batches (simulate parallel agent execution)
            batch_size = min(3, len(files_to_create))  # Up to 3 files at a time
            file_progress = {}  # Track progress for each file

            # Initialize all files as "queued"
            for i, file_info in enumerate(files_to_create):
                file_path = file_info.get("path", f"file_{i}.py")
                file_progress[file_path] = {
                    "status": "queued",
                    "progress": 0,
                    "purpose": file_info.get("purpose", "Implementation"),
                }

            # Process files in parallel batches with progressive streaming
            for batch_start in range(0, len(files_to_create), batch_size):
                batch_end = min(batch_start + batch_size, len(files_to_create))
                batch = files_to_create[batch_start:batch_end]

                # Mark batch files as "in_progress"
                for file_info in batch:
                    file_path = file_info.get("path", f"file_{batch_start}.py")
                    file_progress[file_path]["status"] = "generating"
                    file_progress[file_path]["progress"] = 0

                # Simulate progressive generation for all files in batch
                for progress_step in range(0, 101, 25):
                    # Build parallel streaming content showing all active files
                    parallel_content = f"üîÑ Generating {len(batch)} files in parallel:\n\n"

                    for file_info in batch:
                        file_path = file_info.get("path", f"file.py")
                        purpose = file_info.get("purpose", "Implementation")
                        file_progress[file_path]["progress"] = progress_step

                        # Progress bar for each file
                        bar_length = 20
                        filled = int(bar_length * progress_step / 100)
                        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)

                        parallel_content += f"üìÑ {file_path}\n"
                        parallel_content += f"   [{bar}] {progress_step}%\n"
                        parallel_content += f"   {purpose}\n\n"

                    # Show overall progress
                    completed = sum(1 for f in file_progress.values() if f["status"] == "completed")
                    total = len(files_to_create)
                    parallel_content += f"\nüìä Overall: {completed}/{total} files completed"

                    yield self._create_update("coder", "streaming", {
                        "streaming_files": [f.get("path", "file.py") for f in batch],
                        "streaming_progress": f"{batch_start + len(batch)}/{len(files_to_create)}",
                        "parallel_file_progress": {f.get("path", "file.py"): file_progress.get(f.get("path", "file.py"), {}) for f in batch},
                        "message": f"Generating batch {batch_start // batch_size + 1}...",
                        "streaming_content": parallel_content,
                        "is_parallel": True,
                        "batch_info": {
                            "current": batch_start // batch_size + 1,
                            "total": (len(files_to_create) + batch_size - 1) // batch_size,
                            "files_in_batch": len(batch),
                        }
                    })
                    await asyncio.sleep(0.2)

                # Mark batch files as completed
                for file_info in batch:
                    file_path = file_info.get("path", f"file.py")
                    file_progress[file_path]["status"] = "completed"
                    file_progress[file_path]["progress"] = 100

            coder_result = coder_node(state)
            agent_times["coder"] = time.time() - coder_start
            completed_agents.append("coder")
            state.update(coder_result)

            # Build detailed summary of generated files
            generated_artifacts = coder_result.get("coder_output", {}).get("artifacts", [])
            coder_summary = f"‚úÖ Generated {len(generated_artifacts)} files:\n\n"
            for artifact in generated_artifacts[:5]:
                filename = artifact.get("filename", "unknown")
                description = artifact.get("description", "")
                language = artifact.get("language", "")
                saved = artifact.get("saved", False)
                status = "‚úÖ" if saved else "‚è≥"
                coder_summary += f"{status} {filename}"
                if language:
                    coder_summary += f" [{language}]"
                if description:
                    coder_summary += f"\n   {description}"
                coder_summary += "\n"
            if len(generated_artifacts) > 5:
                coder_summary += f"\n... and {len(generated_artifacts) - 5} more files"

            yield self._create_update("coder", "completed", {
                "coder_output": coder_result.get("coder_output"),
                "artifacts": generated_artifacts,
                "execution_time": agent_times["coder"],
                "completed_agents": completed_agents.copy(),
                "streaming_content": coder_summary,
            })

            # ==================== PHASE 4: QUALITY GATES ====================
            complexity = supervisor_analysis.get("complexity", "moderate")

            # Run quality gates
            for gate_name, gate_func in [
                ("reviewer", reviewer_node),
                ("qa_gate", qa_gate_node),
                ("security_gate", security_gate_node),
            ]:
                yield self._create_update(gate_name, "starting", {
                    "message": f"Running {self._get_agent_info(gate_name)['title']}...",
                })

                gate_start = time.time()
                gate_result = gate_func(state)
                agent_times[gate_name] = time.time() - gate_start
                completed_agents.append(gate_name)
                state.update(gate_result)

                # Create streaming content for gate results
                gate_content = self._format_gate_result(gate_name, gate_result)

                yield self._create_update(gate_name, "completed", {
                    "result": gate_result,
                    "execution_time": agent_times[gate_name],
                    "completed_agents": completed_agents.copy(),
                    "streaming_content": gate_content,
                })

            # ==================== PHASE 5: AGGREGATION ====================
            yield self._create_update("aggregator", "starting", {
                "message": "Aggregating results...",
            })

            agg_start = time.time()
            agg_result = quality_aggregator_node(state)
            agent_times["aggregator"] = time.time() - agg_start
            completed_agents.append("aggregator")
            state.update(agg_result)

            all_passed = (
                state.get("security_passed", True) and
                state.get("qa_passed", True) and
                state.get("review_approved", True)
            )

            agg_content = f"Quality Gate Results:\n"
            agg_content += f"  ‚Ä¢ Security: {'‚úÖ Passed' if state.get('security_passed', True) else '‚ùå Failed'}\n"
            agg_content += f"  ‚Ä¢ QA Tests: {'‚úÖ Passed' if state.get('qa_passed', True) else '‚ùå Failed'}\n"
            agg_content += f"  ‚Ä¢ Review: {'‚úÖ Approved' if state.get('review_approved', True) else '‚ùå Rejected'}"

            yield self._create_update("aggregator", "completed", {
                "all_gates_passed": all_passed,
                "security_passed": state.get("security_passed"),
                "tests_passed": state.get("tests_passed"),
                "review_approved": state.get("review_approved"),
                "execution_time": agent_times["aggregator"],
                "streaming_content": agg_content,
            })

            # ==================== PHASE 6: HITL FINAL APPROVAL ====================
            hitl_approved = True  # Default to approved if no HITL needed

            if complexity in ["complex", "critical"] or not all_passed:
                hitl_request_id = f"final_review_{workflow_id}"

                # Create and register HITL request
                hitl_request = self._create_hitl_request(
                    request_id=hitl_request_id,
                    workflow_id=workflow_id,
                    stage_id="final_approval",
                    checkpoint_type=HITLCheckpointType.APPROVAL if all_passed else HITLCheckpointType.REVIEW,
                    title="Final Review Required" if all_passed else "Issues Found - Review Required",
                    description="Please review the generated code before saving.",
                    content={
                        "type": "code_review",
                        "artifacts_count": len(state.get("final_artifacts", [])),
                        "quality_summary": {
                            "security_passed": state.get("security_passed"),
                            "tests_passed": state.get("tests_passed"),
                            "review_approved": state.get("review_approved"),
                        }
                    },
                    priority="critical" if not all_passed else "high",
                    allow_skip=all_passed
                )

                yield self._create_update("hitl", "awaiting_approval", {
                    "hitl_request": hitl_request.model_dump(),
                    "message": "Waiting for your approval...",
                    "streaming_content": f"Human Review Required\n{hitl_request.title}\n\nPlease approve or provide feedback.",
                })

                # Wait for HITL response with timeout
                max_wait_time = 300  # 5 minutes max wait
                poll_interval = 2  # Check every 2 seconds
                waited = 0
                hitl_approved = False
                hitl_action = None
                hitl_feedback = None
                last_heartbeat = 0

                logger.info(f"[HITL] Waiting for response: {hitl_request_id}")

                while waited < max_wait_time:
                    await asyncio.sleep(poll_interval)
                    waited += poll_interval

                    # Send periodic heartbeat to keep SSE connection alive
                    if waited - last_heartbeat >= 10:
                        last_heartbeat = waited
                        yield self._create_update("hitl", "waiting", {
                            "message": f"Waiting for human approval... ({waited}s / {max_wait_time}s)",
                            "wait_time": waited,
                            "max_wait_time": max_wait_time,
                            "streaming_content": f"‚è≥ Waiting for human approval ({waited}s)",
                        })

                    # Check if response was submitted
                    if hitl_request_id in self.hitl_manager._pending_requests:
                        request = self.hitl_manager._pending_requests[hitl_request_id]
                        # Check if status is no longer pending (enum comparison)
                        status_value = request.status.value if hasattr(request.status, 'value') else str(request.status)
                        response_action = getattr(request, 'response_action', None)
                        logger.info(f"[HITL] Checking status: {status_value}, response_action: {response_action}")

                        # Check for response: either status changed OR response_action was set
                        # Note: RETRY action keeps status as PENDING but sets response_action
                        has_response = (
                            status_value != "pending" or
                            (response_action is not None and response_action != "")
                        )

                        if has_response:
                            hitl_action = request.response_action
                            hitl_feedback = request.response_feedback
                            hitl_approved = hitl_action in ["approve", "confirm"]

                            logger.info(f"[HITL] Response received: action={hitl_action}, approved={hitl_approved}")

                            # Determine response status for UI
                            if hitl_action == "retry":
                                hitl_status = "retry_requested"
                                status_emoji = "üîÑ"
                                status_text = "Retry Requested"
                            elif hitl_action == "reject":
                                hitl_status = "rejected"
                                status_emoji = "‚ùå"
                                status_text = "Rejected"
                            elif hitl_approved:
                                hitl_status = "approved"
                                status_emoji = "‚úÖ"
                                status_text = "Approved"
                            else:
                                hitl_status = "completed"
                                status_emoji = "üìã"
                                status_text = hitl_action.upper() if hitl_action else "Unknown"

                            yield self._create_update("hitl", hitl_status, {
                                "action": hitl_action,
                                "feedback": hitl_feedback,
                                "approved": hitl_approved,
                                "response_status": hitl_status,
                                "streaming_content": f"{status_emoji} Human Response: {status_text}\n{hitl_feedback or 'No feedback provided'}",
                            })
                            break
                    else:
                        # Request was removed - check stored responses
                        if hitl_request_id in self.hitl_manager._responses:
                            response = self.hitl_manager._responses[hitl_request_id]
                            hitl_action = response.action.value if hasattr(response.action, 'value') else str(response.action)
                            hitl_feedback = response.feedback
                            hitl_approved = hitl_action in ["approve", "confirm"]

                            logger.info(f"[HITL] Found in responses: action={hitl_action}, approved={hitl_approved}")

                            yield self._create_update("hitl", "completed", {
                                "action": hitl_action,
                                "feedback": hitl_feedback,
                                "approved": hitl_approved,
                                "streaming_content": f"Human Response: {hitl_action.upper()}\n{hitl_feedback or 'No feedback provided'}",
                            })
                            break
                        else:
                            logger.warning(f"[HITL] Request removed but no response found: {hitl_request_id}")
                            break

                # Check if timeout
                if waited >= max_wait_time and not hitl_action:
                    logger.warning(f"[HITL] Timeout waiting for response: {hitl_request_id}")
                    yield self._create_update("hitl", "timeout", {
                        "message": "HITL request timed out",
                        "streaming_content": "‚ö†Ô∏è Timeout: No response received within 5 minutes",
                    })
                    hitl_approved = True  # Default to approved on timeout

                # Handle rejection/retry
                if not hitl_approved and hitl_action in ["reject", "retry"]:
                    total_time = time.time() - start_time

                    if hitl_action == "retry":
                        yield self._create_update("hitl", "retry_requested", {
                            "action": hitl_action,
                            "feedback": hitl_feedback,
                            "approved": False,
                            "response_status": "retry_requested",
                            "streaming_content": f"üîÑ Retry Requested\n\nFeedback: {hitl_feedback or 'None'}",
                        })

                        # Send workflow stopped message
                        yield self._create_update("workflow", "stopped", {
                            "reason": "retry_requested",
                            "action": hitl_action,
                            "feedback": hitl_feedback,
                            "total_execution_time": round(total_time, 2),
                            "streaming_content": f"üîÑ Workflow Stopped - Retry Requested\n\nPlease submit your request again with the feedback incorporated.\n\nFeedback: {hitl_feedback or 'None'}\n\nTime elapsed: {total_time:.1f}s",
                            "message": "Workflow stopped - retry requested",
                            "is_final": True,
                        })
                    else:
                        yield self._create_update("hitl", "rejected", {
                            "action": hitl_action,
                            "feedback": hitl_feedback,
                            "approved": False,
                            "response_status": "rejected",
                            "streaming_content": f"‚ùå Rejected\n\nFeedback: {hitl_feedback or 'None'}",
                        })

                        # Send workflow stopped message
                        yield self._create_update("workflow", "stopped", {
                            "reason": "rejected_by_user",
                            "action": hitl_action,
                            "feedback": hitl_feedback,
                            "total_execution_time": round(total_time, 2),
                            "streaming_content": f"‚ùå Workflow Rejected by User\n\nFeedback: {hitl_feedback or 'None'}\n\nPlease submit a new request with updated requirements.\n\nTime elapsed: {total_time:.1f}s",
                            "message": "Workflow rejected by user",
                            "is_final": True,
                        })

                    logger.info(f"[Workflow] Stopped due to {hitl_action}: {hitl_feedback}")
                    return  # Stop workflow execution

            # ==================== PHASE 7: PERSISTENCE ====================
            yield self._create_update("persistence", "starting", {
                "message": "Saving files to workspace...",
            })

            # Collect all artifacts from coder output
            coder_output = state.get("coder_output", {})
            artifacts_to_save = coder_output.get("artifacts", [])
            if not artifacts_to_save:
                artifacts_to_save = state.get("artifacts", [])

            # Set final_artifacts for persistence
            state["final_artifacts"] = artifacts_to_save
            state["workflow_status"] = "completed" if all_passed else "completed_with_issues"

            persist_start = time.time()
            persist_result = persistence_node(state)
            agent_times["persistence"] = time.time() - persist_start
            completed_agents.append("persistence")

            # Use artifacts_to_save for display since persistence doesn't return them
            saved_files = artifacts_to_save
            persist_content = f"üíæ Saved {len(saved_files)} files to {project_name}/:\n"
            for artifact in saved_files[:10]:
                filename = artifact.get('filename', 'unknown')
                desc = artifact.get('description', '')
                saved = '‚úÖ' if artifact.get('saved', True) else '‚ùå'
                persist_content += f"  {saved} {filename}"
                if desc:
                    persist_content += f" - {desc}"
                persist_content += "\n"
            if len(saved_files) > 10:
                persist_content += f"  ... and {len(saved_files) - 10} more files\n"
            persist_content += f"\nüìÇ Project location: {project_dir}"

            yield self._create_update("persistence", "completed", {
                "saved_files": saved_files,
                "artifacts": saved_files,
                "project_name": project_name,
                "project_dir": project_dir,
                "execution_time": agent_times["persistence"],
                "streaming_content": persist_content,
            })

            # ==================== WORKFLOW COMPLETE ====================
            total_time = time.time() - start_time

            summary = f"‚úÖ Workflow Complete in {total_time:.1f}s\n\n"
            summary += "Agent Execution Times:\n"
            for agent, t in agent_times.items():
                summary += f"  ‚Ä¢ {self._get_agent_info(agent)['title']}: {t:.1f}s\n"

            yield self._create_update("workflow", "completed", {
                "workflow_id": workflow_id,
                "total_execution_time": round(total_time, 2),
                "agent_execution_times": {k: round(v, 2) for k, v in agent_times.items()},
                "completed_agents": completed_agents,
                "final_artifacts": state.get("final_artifacts", []),
                "streaming_content": summary,
                "message": f"Workflow completed in {total_time:.1f}s",
            })

        except Exception as e:
            logger.error(f"‚ùå Workflow failed: {e}", exc_info=True)
            yield self._create_update("error", "error", {
                "error": str(e),
                "workflow_id": workflow_id,
                "streaming_content": f"‚ùå Error: {str(e)}",
            })

    def _generate_project_name(
        self,
        user_request: str,
        supervisor_analysis: Dict,
        architecture: Dict
    ) -> str:
        """Generate a meaningful project name from user request analysis

        Priority:
        1. Architecture project_name if meaningful
        2. Extract from user request keywords
        3. Use task type from supervisor analysis
        """
        import re

        # 1. Try architecture project_name first
        arch_name = architecture.get("project_name", "")
        if arch_name and arch_name.lower() not in ["project", "app", "application", "code"]:
            return arch_name

        # 2. Extract meaningful keywords from user request
        request_lower = user_request.lower()

        # Common Korean keywords to project names
        korean_keywords = {
            "Í≥ÑÏÇ∞Í∏∞": "calculator",
            "ÏÇ¨ÏπôÏó∞ÏÇ∞": "calculator",
            "Ìï†Ïùº": "todo",
            "Ìà¨Îëê": "todo",
            "Î©îÎ™®": "memo",
            "ÏùºÍ∏∞": "diary",
            "Í≤åÏãúÌåê": "board",
            "ÏáºÌïë": "shopping",
            "Ïû•Î∞îÍµ¨Îãà": "cart",
            "Î°úÍ∑∏Ïù∏": "auth",
            "ÌöåÏõêÍ∞ÄÏûÖ": "signup",
            "Ï±ÑÌåÖ": "chat",
            "ÎÇ†Ïî®": "weather",
            "Îâ¥Ïä§": "news",
            "Î∏îÎ°úÍ∑∏": "blog",
            "Ìè¨Ìä∏Ìè¥Î¶¨Ïò§": "portfolio",
            "ÎåÄÏãúÎ≥¥Îìú": "dashboard",
            "Í¥ÄÎ¶¨Ïûê": "admin",
            "api": "api",
            "ÏÑúÎ≤Ñ": "server",
            "ÌÅ¨Î°§Îü¨": "crawler",
            "Ïä§ÌÅ¨ÎûòÌçº": "scraper",
            "Î¥á": "bot",
            "Í≤åÏûÑ": "game",
            "ÌÄ¥Ï¶à": "quiz",
        }

        # Check Korean keywords
        for kor, eng in korean_keywords.items():
            if kor in request_lower:
                # Add suffix based on tech stack
                tech = architecture.get("tech_stack", {}).get("language", "")
                if tech.lower() in ["javascript", "typescript", "react"]:
                    return f"{eng}-app"
                elif "web" in request_lower or "Ïõπ" in request_lower:
                    return f"{eng}-web"
                return eng

        # English keyword patterns
        english_patterns = [
            (r'\b(calculator|calc)\b', 'calculator'),
            (r'\b(todo|task)\s*(list|app)?\b', 'todo-app'),
            (r'\b(weather)\s*(app)?\b', 'weather-app'),
            (r'\b(chat)\s*(app|bot)?\b', 'chat-app'),
            (r'\b(blog)\b', 'blog'),
            (r'\b(dashboard)\b', 'dashboard'),
            (r'\b(api|rest|server)\b', 'api-server'),
            (r'\b(game)\b', 'game'),
            (r'\b(portfolio)\b', 'portfolio'),
            (r'\b(ecommerce|shop|store)\b', 'ecommerce'),
            (r'\b(auth|login)\b', 'auth-system'),
        ]

        for pattern, name in english_patterns:
            if re.search(pattern, request_lower):
                return name

        # 3. Use task type from supervisor analysis
        task_type = supervisor_analysis.get("task_type", "")
        if task_type and task_type not in ["general", "implementation"]:
            return f"{task_type}-project"

        # 4. Try to extract first meaningful noun from request
        # Remove common words and get first significant word
        stopwords = {'a', 'an', 'the', 'create', 'make', 'build', 'develop', 'write',
                     'implement', 'generate', 'please', 'want', 'need', 'simple',
                     'basic', 'new', 'web', 'app', 'application', 'ÎßåÎì§Ïñ¥', 'ÏÉùÏÑ±',
                     'Í∞úÎ∞ú', 'Íµ¨ÌòÑ', 'ÏûëÏÑ±', 'Ï§ò', 'Ï£ºÏÑ∏Ïöî', 'Ìï¥Ï§ò', 'Ìï¥Ï£ºÏÑ∏Ïöî'}

        words = re.findall(r'[a-zA-ZÍ∞Ä-Ìû£]+', user_request)
        for word in words:
            word_lower = word.lower()
            if len(word) >= 3 and word_lower not in stopwords:
                return word_lower[:20]  # Limit length

        # 5. Fallback to timestamp-based name
        return f"project-{int(time.time()) % 10000}"

    def _generate_code_preview(self, file_path: str, purpose: str) -> str:
        """Generate a preview of code being created"""
        ext = file_path.split('.')[-1] if '.' in file_path else 'py'

        if ext in ['py', 'python']:
            return f'''# {file_path}
# Purpose: {purpose}

def main():
    """Main entry point"""
    pass

if __name__ == "__main__":
    main()'''
        elif ext in ['ts', 'tsx', 'js', 'jsx']:
            return f'''// {file_path}
// Purpose: {purpose}

export function main() {{
  // Implementation
}}'''
        else:
            return f"# {file_path}\n# {purpose}"

    def _format_gate_result(self, gate_name: str, result: Dict) -> str:
        """Format quality gate result for streaming display"""
        if gate_name == "reviewer":
            feedback = result.get("review_feedback", {})
            approved = result.get("review_approved", feedback.get("approved", True))
            score = feedback.get("quality_score", 0.8)
            issues = feedback.get("issues", [])
            suggestions = feedback.get("suggestions", [])
            critique = feedback.get("critique", "Code quality is acceptable")

            content = f"Code Review: {'‚úÖ Approved' if approved else '‚ö†Ô∏è Needs Attention'}\n"
            content += f"Quality Score: {score:.0%}\n"
            if critique:
                content += f"Summary: {critique}\n"
            if issues:
                content += f"\nIssues ({len(issues)}):\n"
                for issue in issues[:3]:
                    content += f"  ‚ö†Ô∏è {issue}\n"
            if suggestions:
                content += f"\nSuggestions ({len(suggestions)}):\n"
                for sug in suggestions[:3]:
                    content += f"  üí° {sug}\n"
            return content.strip()

        elif gate_name == "qa_gate":
            qa_results = result.get("qa_results", {})
            passed = result.get("qa_passed", qa_results.get("passed", True))
            checks = qa_results.get("checks", {})
            passed_count = sum(1 for c in checks.values() if c.get("passed", False))
            total_count = len(checks)
            content = f"QA Tests: {'‚úÖ All Passed' if passed else '‚ö†Ô∏è Some Issues'}\n"
            content += f"Checks: {passed_count}/{total_count} passed\n"
            for name, check in checks.items():
                status = '‚úÖ' if check.get('passed', False) else '‚ùå'
                content += f"  {status} {name}: {check.get('message', '')}\n"
            return content.strip()

        elif gate_name == "security_gate":
            passed = result.get("security_passed", True)
            findings = result.get("security_findings", [])
            critical = [f for f in findings if f.get("severity") in ["critical", "high"]]
            medium = [f for f in findings if f.get("severity") == "medium"]

            content = f"Security Scan: {'‚úÖ Passed' if passed else '‚ö†Ô∏è Issues Found'}\n"
            if findings:
                content += f"Findings: {len(findings)} total ({len(critical)} critical/high, {len(medium)} medium)\n"
                for finding in critical[:3]:
                    content += f"  üî¥ [{finding.get('severity')}] {finding.get('category', 'unknown')}\n"
                    content += f"     {finding.get('description', 'No description')}\n"
            else:
                content += "No security vulnerabilities detected\n"
            return content.strip()

        return f"{gate_name}: Complete"

    def _create_update(
        self,
        node: str,
        status: str,
        data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Create standardized update for frontend"""
        agent_info = self._get_agent_info(node)

        return {
            "node": node,
            "status": status,
            "agent_title": agent_info["title"],
            "agent_description": agent_info["description"],
            "agent_icon": agent_info["icon"],
            "updates": data,
            "timestamp": datetime.utcnow().isoformat(),
        }


# Global enhanced workflow instance
enhanced_workflow = EnhancedWorkflow()
