"""State/LLM/Bridge Verification

Tests state management, LLM client behavior, and backend bridge communication.
Verifies:
- State consistency and updates
- LLM failover and error handling
- Event streaming completeness
"""

import asyncio
import sys
from pathlib import Path
from typing import Dict, Any
import tempfile

sys.path.insert(0, str(Path(__file__).parent.parent))

from core.state import AgenticState, TaskStatus, increment_iteration, add_error
from core.llm_client import DualEndpointLLMClient, EndpointConfig


def test_state_structure():
    """Test AgenticState TypedDict structure"""
    print("\nğŸ§ª Testing AgenticState Structure...")

    # Create a valid state
    state: AgenticState = {
        "task_description": "Test task",
        "task_id": "test_001",
        "task_type": "coding",
        "workspace": "/tmp/test",
        "iteration": 0,
        "max_iterations": 50,
        "task_status": "pending",
        "context": {},
        "tool_calls": [],
        "errors": [],
        "requires_sub_agents": False,
        "should_continue": True
    }

    # Check all required fields exist
    required_fields = [
        "task_description", "task_id", "task_type", "workspace",
        "iteration", "max_iterations", "task_status", "context",
        "tool_calls", "errors", "requires_sub_agents", "should_continue"
    ]

    missing_fields = []
    for field in required_fields:
        if field not in state:
            missing_fields.append(field)

    if missing_fields:
        print(f"   âŒ Missing required fields: {missing_fields}")
        return False
    else:
        print(f"   âœ… All required fields present ({len(required_fields)} fields)")

    # Check field types
    print(f"   âœ… task_description: {type(state['task_description']).__name__}")
    print(f"   âœ… iteration: {type(state['iteration']).__name__}")
    print(f"   âœ… context: {type(state['context']).__name__}")
    print(f"   âœ… tool_calls: {type(state['tool_calls']).__name__}")

    print(f"   âœ… AgenticState structure test passed")
    return True


def test_state_helper_functions():
    """Test state manipulation helper functions"""
    print("\nğŸ§ª Testing State Helper Functions...")

    state: AgenticState = {
        "task_description": "Test",
        "task_id": "test_001",
        "task_type": "coding",
        "workspace": "/tmp",
        "iteration": 0,
        "max_iterations": 50,
        "task_status": "pending",
        "context": {},
        "tool_calls": [],
        "errors": [],
        "requires_sub_agents": False,
        "should_continue": True
    }

    # Test increment_iteration
    original_iteration = state["iteration"]
    state = increment_iteration(state)
    if state["iteration"] == original_iteration + 1:
        print(f"   âœ… increment_iteration: {original_iteration} â†’ {state['iteration']}")
    else:
        print(f"   âŒ increment_iteration failed")
        return False

    # Test add_error
    test_error = "Test error message"
    state = add_error(state, test_error)
    # Error is stored as dict with "message" key
    if len(state["errors"]) > 0 and state["errors"][0]["message"] == test_error:
        print(f"   âœ… add_error: Error added to list (total: {len(state['errors'])})")
    else:
        print(f"   âŒ add_error failed")
        return False

    # Test multiple errors
    state = add_error(state, "Second error")
    if len(state["errors"]) == 2:
        print(f"   âœ… Multiple errors: {len(state['errors'])} errors tracked")
    else:
        print(f"   âŒ Multiple errors failed")
        return False

    # Test error structure
    first_error = state["errors"][0]
    if "message" in first_error and "timestamp" in first_error and "iteration" in first_error:
        print(f"   âœ… Error structure: message, timestamp, iteration present")
    else:
        print(f"   âŒ Error structure incomplete")
        return False

    print(f"   âœ… State helper functions test passed")
    return True


def test_task_status_enum():
    """Test TaskStatus enum values"""
    print("\nğŸ§ª Testing TaskStatus Enum...")

    # Check TaskStatus has expected values
    expected_statuses = ["PENDING", "IN_PROGRESS", "COMPLETED", "FAILED"]

    found_statuses = []
    for status_name in expected_statuses:
        if hasattr(TaskStatus, status_name):
            status = getattr(TaskStatus, status_name)
            found_statuses.append(status_name)
            print(f"   âœ… TaskStatus.{status_name} = {status.value}")
        else:
            print(f"   âŒ Missing TaskStatus.{status_name}")
            return False

    if len(found_statuses) == len(expected_statuses):
        print(f"   âœ… All TaskStatus values present")
    else:
        print(f"   âŒ Some TaskStatus values missing")
        return False

    print(f"   âœ… TaskStatus enum test passed")
    return True


async def test_llm_client_initialization():
    """Test LLM client initialization"""
    print("\nğŸ§ª Testing LLM Client Initialization...")

    try:
        # Create endpoint configs
        endpoints = [
            EndpointConfig(url="http://localhost:8001/v1", name="primary"),
            EndpointConfig(url="http://localhost:8002/v1", name="secondary")
        ]

        # Initialize LLM client
        llm_client = DualEndpointLLMClient(endpoints)

        # Check that client has required attributes
        if not hasattr(llm_client, 'endpoints'):
            print(f"   âŒ Missing endpoints")
            return False

        if not hasattr(llm_client, 'clients'):
            print(f"   âŒ Missing clients")
            return False

        if len(llm_client.endpoints) != 2:
            print(f"   âŒ Expected 2 endpoints, got {len(llm_client.endpoints)}")
            return False

        print(f"   âœ… Endpoints configured: {len(llm_client.endpoints)}")
        print(f"   âœ… Primary: {llm_client.endpoints[0].name} ({llm_client.endpoints[0].url})")
        print(f"   âœ… Secondary: {llm_client.endpoints[1].name} ({llm_client.endpoints[1].url})")

        # Check methods exist
        if not hasattr(llm_client, 'chat_completion'):
            print(f"   âŒ Missing chat_completion method")
            return False

        if not hasattr(llm_client, 'stream_chat_completion'):
            print(f"   âš ï¸  Missing stream_chat_completion method (might not be implemented)")
            # Not a failure - streaming might not be implemented yet

        print(f"   âœ… Required methods present (chat_completion)")
        print(f"   âœ… LLM client initialization passed")
        return True

    except Exception as e:
        print(f"   âŒ LLM client initialization failed: {e}")
        import traceback
        traceback.print_exc()
        return False


async def test_llm_failover_logic():
    """Test LLM failover behavior (without actual API calls)"""
    print("\nğŸ§ª Testing LLM Failover Logic...")

    try:
        endpoints = [
            EndpointConfig(url="http://localhost:8001/v1", name="primary"),
            EndpointConfig(url="http://localhost:8002/v1", name="secondary")
        ]
        llm_client = DualEndpointLLMClient(endpoints)

        # Check that failover attributes exist
        if hasattr(llm_client, '_failover_enabled'):
            print(f"   âœ… Failover mechanism exists")
        else:
            print(f"   âš ï¸  No explicit failover flag (might be implicit)")

        # Check timeout settings
        if hasattr(llm_client, 'timeout'):
            print(f"   âœ… Timeout configured: {llm_client.timeout}s")
        else:
            print(f"   âš ï¸  No explicit timeout setting")

        # Check retry logic
        if hasattr(llm_client, 'max_retries') or hasattr(llm_client, '_max_retries'):
            max_retries = getattr(llm_client, 'max_retries', getattr(llm_client, '_max_retries', None))
            print(f"   âœ… Retry logic: max {max_retries} retries")
        else:
            print(f"   âš ï¸  No explicit retry configuration")

        print(f"   âœ… LLM failover logic test passed")
        return True

    except Exception as e:
        print(f"   âŒ Failover logic test failed: {e}")
        return False


def test_context_management():
    """Test context storage and retrieval in state"""
    print("\nğŸ§ª Testing Context Management...")

    state: AgenticState = {
        "task_description": "Test",
        "task_id": "test_001",
        "task_type": "coding",
        "workspace": "/tmp",
        "iteration": 0,
        "max_iterations": 50,
        "task_status": "pending",
        "context": {},
        "tool_calls": [],
        "errors": [],
        "requires_sub_agents": False,
        "should_continue": True
    }

    # Test storing various context types
    state["context"]["plan"] = {"steps": ["step1", "step2"]}
    state["context"]["completed_steps"] = ["step1"]
    state["context"]["current_file"] = "test.py"
    state["context"]["metadata"] = {"complexity": "simple"}

    # Verify storage
    if "plan" in state["context"]:
        print(f"   âœ… Context storage: plan stored")
    else:
        print(f"   âŒ Context storage: plan not stored")
        return False

    if "completed_steps" in state["context"]:
        print(f"   âœ… Context storage: completed_steps stored")
    else:
        print(f"   âŒ Context storage: completed_steps not stored")
        return False

    # Test retrieval
    plan = state["context"].get("plan")
    if plan and "steps" in plan:
        print(f"   âœ… Context retrieval: plan retrieved correctly")
    else:
        print(f"   âŒ Context retrieval: plan not retrieved correctly")
        return False

    # Test nested access
    complexity = state["context"].get("metadata", {}).get("complexity")
    if complexity == "simple":
        print(f"   âœ… Nested context: metadata.complexity = {complexity}")
    else:
        print(f"   âŒ Nested context failed")
        return False

    print(f"   âœ… Context management test passed")
    return True


def test_tool_calls_tracking():
    """Test tool calls tracking in state"""
    print("\nğŸ§ª Testing Tool Calls Tracking...")

    state: AgenticState = {
        "task_description": "Test",
        "task_id": "test_001",
        "task_type": "coding",
        "workspace": "/tmp",
        "iteration": 0,
        "max_iterations": 50,
        "task_status": "pending",
        "context": {},
        "tool_calls": [],
        "errors": [],
        "requires_sub_agents": False,
        "should_continue": True
    }

    # Add tool calls
    tool_call_1 = {
        "action": "WRITE_FILE",
        "parameters": {"file_path": "test.py", "content": "print('hello')"},
        "result": {"success": True}
    }

    tool_call_2 = {
        "action": "READ_FILE",
        "parameters": {"file_path": "test.py"},
        "result": {"success": True, "output": "print('hello')"}
    }

    state["tool_calls"].append(tool_call_1)
    state["tool_calls"].append(tool_call_2)

    # Verify tracking
    if len(state["tool_calls"]) == 2:
        print(f"   âœ… Tool calls tracked: {len(state['tool_calls'])} calls")
    else:
        print(f"   âŒ Tool calls tracking failed")
        return False

    # Verify structure
    first_call = state["tool_calls"][0]
    if "action" in first_call and "parameters" in first_call and "result" in first_call:
        print(f"   âœ… Tool call structure: action, parameters, result present")
    else:
        print(f"   âŒ Tool call structure incomplete")
        return False

    # Test filtering
    write_calls = [call for call in state["tool_calls"] if call["action"] == "WRITE_FILE"]
    if len(write_calls) == 1:
        print(f"   âœ… Tool call filtering: Found {len(write_calls)} WRITE_FILE calls")
    else:
        print(f"   âŒ Tool call filtering failed")
        return False

    print(f"   âœ… Tool calls tracking test passed")
    return True


async def main():
    """Run all State/LLM/Bridge tests"""
    print("="*70)
    print("ğŸ” STATE/LLM/BRIDGE VERIFICATION")
    print("="*70)

    results = {}

    # State tests
    results['state_structure'] = test_state_structure()
    results['state_helpers'] = test_state_helper_functions()
    results['task_status'] = test_task_status_enum()
    results['context_management'] = test_context_management()
    results['tool_tracking'] = test_tool_calls_tracking()

    # LLM tests
    results['llm_initialization'] = await test_llm_client_initialization()
    results['llm_failover'] = await test_llm_failover_logic()

    # Summary
    print("\n" + "="*70)
    print("ğŸ“Š SUMMARY")
    print("="*70)

    all_passed = True
    for test_name, passed in results.items():
        status = "âœ… PASSED" if passed else "âŒ FAILED"
        print(f"   {test_name.ljust(25)}: {status}")
        if not passed:
            all_passed = False

    print("="*70)

    if all_passed:
        print("\nâœ… All State/LLM/Bridge tests passed!")
        print("\nâ„¹ï¸  Note: Backend bridge tests require running app")
        print("   These tests verify structure and logic only.")
        print("   Full integration requires manual testing with live system.")
        return 0
    else:
        print("\nâŒ Some State/LLM/Bridge tests failed")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
