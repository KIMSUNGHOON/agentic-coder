# UI/UX ê°œì„  ê³„íš (Improvement Plan)

**ì‘ì„±ì¼**: 2026-01-15
**ìš°ì„ ìˆœìœ„**: ğŸ”¥ CRITICAL

## ğŸ” í˜„ì¬ ë¬¸ì œì  ë¶„ì„

### 1. âŒ í•­ìƒ "Max iterations reached" ì—ëŸ¬ ë°œìƒ

**ì¦ìƒ**:
```
Error | Task failed: Max iterations reached
```
- ëª¨ë“  ì‘ì—…ì´ 50 iterationsê¹Œì§€ ì‹¤í–‰ë¨
- COMPLETE ì•¡ì…˜ì„ ê°ì§€í•˜ì§€ ëª»í•¨

**ì›ì¸ ë¶„ì„**:
```python
# workflows/general_workflow.py:227-230
if action.get("action") == "COMPLETE":
    state["task_status"] = TaskStatus.COMPLETED.value
    state["task_result"] = action.get("summary", "Task completed")
    state["should_continue"] = False  # âœ… ì—¬ê¸°ì„œ Falseë¡œ ì„¤ì •

# workflows/general_workflow.py:332
state["should_continue"] = True  # âŒ reflect_nodeì—ì„œ ë¬´ì¡°ê±´ Trueë¡œ ë®ì–´ì”€!
```

**ê·¼ë³¸ ì›ì¸**:
- `execute_node`ì—ì„œ COMPLETE ê°ì§€í•˜ê³  `should_continue=False` ì„¤ì •
- í•˜ì§€ë§Œ `reflect_node`ê°€ ë‚˜ì¤‘ì— ì‹¤í–‰ë˜ë©´ì„œ `should_continue=True`ë¡œ ë®ì–´ì”€
- LangGraph ì‹¤í–‰ ìˆœì„œ: execute â†’ reflect â†’ should_continue í™•ì¸
- reflectê°€ executeì˜ ê²°ê³¼ë¥¼ ë¬´ì‹œí•˜ê³  ìˆìŒ!

### 2. âŒ ë§¤ë²ˆ "No session" ìƒíƒœ

**ì¦ìƒ**:
```
Status Bar: No session
```
- ì„¸ì…˜ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ
- ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ì§€ë˜ì§€ ì•ŠìŒ

**ì›ì¸ ë¶„ì„**:
```python
# cli/app.py:166-167
status = self.query_one("#status-bar", StatusBar)
status.update_status("Ready", "healthy")
# âŒ ì„¸ì…˜ ID ì„¤ì • ì½”ë“œ ì—†ìŒ!

# backend_bridge.pyì—ì„œ ì„¸ì…˜ì„ ìƒì„±í•˜ì§€ë§Œ
# UIì— ë°˜ì˜í•˜ì§€ ì•ŠìŒ
```

**ê·¼ë³¸ ì›ì¸**:
- Backendì—ì„œ ì„¸ì…˜ì„ ìƒì„±í•˜ì§€ë§Œ UIì— ì „ë‹¬í•˜ì§€ ì•ŠìŒ
- `status.set_session(session_id)` í˜¸ì¶œ ëˆ„ë½

### 3. âŒ Chat ìœˆë„ìš°ì— ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ

**ì¦ìƒ**:
- í”„ë¡¬í”„íŠ¸ ì…ë ¥ í›„ â†’ ê¸´ ëŒ€ê¸° â†’ ìµœì¢… ê²°ê³¼ë§Œ í‘œì‹œ
- LLMì´ ìƒì„±í•˜ëŠ” í† í°ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì´ì§€ ì•ŠìŒ
- ì§„í–‰ ìƒí™©ì„ ì „í˜€ ì•Œ ìˆ˜ ì—†ìŒ

**í˜„ì¬ ë™ì‘**:
```python
# cli/app.py:241-246
elif update.type == "result":
    # âŒ ìµœì¢… ê²°ê³¼ë§Œ í‘œì‹œ
    if update.data["success"]:
        output = update.data.get("output", "")
        chat.add_message("assistant", str(output))
```

**ë¬¸ì œì **:
- `update.type == "status"` â†’ Progress barì—ë§Œ í‘œì‹œ, Chatì—ëŠ” ì—†ìŒ
- LLM ì‘ë‹µì´ ì™„ë£Œë  ë•Œê¹Œì§€ Chat ì˜ì—­ ì—…ë°ì´íŠ¸ ì—†ìŒ
- ì‚¬ìš©ìëŠ” "ë©ˆì¶˜ ê²ƒì¸ê°€?"ë¼ê³  ìƒê°í•¨

**í•„ìš”í•œ ê²ƒ**:
```python
# ì›í•˜ëŠ” ë™ì‘:
elif update.type == "llm_token":
    # í† í° ë‹¨ìœ„ë¡œ ì‹¤ì‹œê°„ í‘œì‹œ
    chat.append_streaming(update.message)

elif update.type == "node_start":
    # ë…¸ë“œ ì‹œì‘ í‘œì‹œ
    chat.add_status("ê³„íš ìˆ˜ë¦½ ì¤‘...")

elif update.type == "action_executing":
    # ì•¡ì…˜ ì‹¤í–‰ í‘œì‹œ
    chat.add_status("ğŸ”§ Executing: READ_FILE")
```

### 4. âŒ Markdown/ì½”ë“œ ë Œë”ë§ ì—†ìŒ

**ì¦ìƒ**:
- Planning ê²°ê³¼ê°€ plain textë¡œë§Œ í‘œì‹œ
- ì½”ë“œ ë¸”ë¡ì´ í¬ë§·íŒ… ì—†ì´ í‘œì‹œ
- TypeScript, Python ë“± syntax highlighting ì—†ìŒ

**í˜„ì¬ ì½”ë“œ**:
```python
# cli/components/chat_panel.py:63
text.append(content, style="white")  # âŒ ë‹¨ìˆœ í…ìŠ¤íŠ¸ë§Œ
```

**ë¬¸ì œì **:
- Richì˜ `Markdown` ê¸°ëŠ¥ ë¯¸ì‚¬ìš©
- Richì˜ `Syntax` ê¸°ëŠ¥ ë¯¸ì‚¬ìš©
- ì½”ë“œì™€ ì¼ë°˜ í…ìŠ¤íŠ¸ êµ¬ë¶„ ì•ˆ ë¨

**ì˜ˆì‹œ**:
```
# í˜„ì¬ í‘œì‹œ:
Assistant: Here is the plan: 1. Read file 2. Analyze code ```typescript const x = 1; ```

# ì›í•˜ëŠ” í‘œì‹œ:
Assistant: Here is the plan:
1. Read file
2. Analyze code

â”Œâ”€ TypeScript â”€â”€â”€â”€â”
â”‚ const x = 1;    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 5. âŒ í”„ë¡¬í”„íŠ¸ ì…ë ¥ í›„ í”¼ë“œë°± ì—†ìŒ

**ì¦ìƒ**:
- ë©”ì‹œì§€ ì…ë ¥ â†’ Enter â†’ ... (ì•„ë¬´ ë°˜ì‘ ì—†ìŒ) â†’ 1ë¶„ í›„ ê²°ê³¼
- "ì²˜ë¦¬ ì¤‘ì¸ê°€? ë©ˆì¶˜ ê±´ê°€?" ì•Œ ìˆ˜ ì—†ìŒ

**í˜„ì¬ ë™ì‘**:
```
1. ì‚¬ìš©ì ë©”ì‹œì§€ ì…ë ¥
2. Progress barë§Œ ì—…ë°ì´íŠ¸ (í•˜ë‹¨)
3. Chat ì˜ì—­ì€ ë³€í™” ì—†ìŒ â† ë¬¸ì œ!
4. ì˜¤ëœ ì‹œê°„ í›„ ê²°ê³¼ í‘œì‹œ
```

**í•„ìš”í•œ ë™ì‘**:
```
1. ì‚¬ìš©ì ë©”ì‹œì§€ ì…ë ¥
2. Chatì— ì¦‰ì‹œ "Processing..." í‘œì‹œ â† ì¶”ê°€ í•„ìš”!
3. ê° ë‹¨ê³„ë³„ë¡œ Chatì— ìƒíƒœ í‘œì‹œ:
   - "ğŸ“‹ Planning..."
   - "ğŸ¤” Analyzing complexity..."
   - "ğŸ”§ Executing tools..."
   - "ğŸ’­ Reflecting on results..."
4. LLM ì‘ë‹µ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
5. ìµœì¢… ê²°ê³¼ í‘œì‹œ
```

---

## âœ… ê°œì„  ëª©ë¡ (Improvement List)

### Priority 1: ğŸ”¥ CRITICAL (ì¦‰ì‹œ ìˆ˜ì • í•„ìš”)

#### 1.1. Fix: reflect_nodeê°€ should_continueë¥¼ ë®ì–´ì“°ëŠ” ë¬¸ì œ

**íŒŒì¼**: `agentic-ai/workflows/general_workflow.py`

**ë¬¸ì œ**:
```python
# Line 332: reflect_node
state["should_continue"] = True  # âŒ ë¬´ì¡°ê±´ True
```

**í•´ê²°**:
```python
# reflect_node ìˆ˜ì •
async def reflect_node(self, state: AgenticState) -> AgenticState:
    # Check if ALREADY completed in execute_node
    if state.get("task_status") == TaskStatus.COMPLETED.value:
        logger.info("âœ… Task is COMPLETED (from execute_node), stopping")
        state["should_continue"] = False  # âœ… ìœ ì§€!
        return state

    # ... rest of reflect logic
```

**ì˜í–¥**: Max iterations ë¬¸ì œ í•´ê²°

#### 1.2. Add: ì„¸ì…˜ ID ìƒì„± ë° UI í‘œì‹œ

**íŒŒì¼**: `agentic-ai/cli/app.py`

**ì¶”ê°€ ìœ„ì¹˜**: `process_message()` ì‹œì‘ ë¶€ë¶„

**ì½”ë“œ**:
```python
async def process_message(self, message: str) -> None:
    # Generate session ID if not exists
    if not hasattr(self, 'session_id') or not self.session_id:
        import uuid
        self.session_id = str(uuid.uuid4())

        # Update status bar
        status = self.query_one("#status-bar", StatusBar)
        status.set_session(self.session_id)

        logger.info(f"ğŸ“ Created session: {self.session_id}")
```

**ì˜í–¥**: "No session" ë¬¸ì œ í•´ê²°

#### 1.3. Add: Chatì— ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ

**íŒŒì¼**: `agentic-ai/cli/app.py` â†’ `process_message()`

**í˜„ì¬**:
```python
if update.type == "status":
    progress.update_progress(...)  # Progress barë§Œ
    log.add_log(...)               # Logë§Œ
```

**ê°œì„ **:
```python
if update.type == "status":
    progress.update_progress(...)
    log.add_log(...)
    chat.add_status(update.message)  # âœ… Chatì—ë„ í‘œì‹œ!

elif update.type == "node_start":
    # ë…¸ë“œ ì‹œì‘ ì•Œë¦¼
    chat.add_status(f"ğŸ”„ {update.message}")

elif update.type == "llm_streaming":
    # LLM í† í° ìŠ¤íŠ¸ë¦¬ë°
    chat.append_streaming(update.message)
```

**ì˜í–¥**: ì‚¬ìš©ìê°€ ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ

### Priority 2: â­ HIGH (UX ê°œì„ )

#### 2.1. Add: ChatPanelì— Markdown ë Œë”ë§

**íŒŒì¼**: `agentic-ai/cli/components/chat_panel.py`

**í˜„ì¬**:
```python
def add_message(self, role: str, content: str) -> None:
    # Plain textë§Œ í‘œì‹œ
    text.append(content, style="white")
```

**ê°œì„ **:
```python
from rich.markdown import Markdown
from rich.syntax import Syntax

def add_message(self, role: str, content: str, format: str = "text") -> None:
    """Add a message with optional formatting

    Args:
        role: user, assistant, system
        content: Message content
        format: "text", "markdown", "code"
    """
    if format == "markdown":
        # Render as Markdown
        md = Markdown(content)
        self.write(Panel(md, border_style="green", ...))

    elif format == "code":
        # Detect language and syntax highlight
        language = self._detect_language(content)
        syntax = Syntax(content, language, theme="monokai")
        self.write(Panel(syntax, border_style="blue", ...))

    else:
        # Plain text (current behavior)
        ...
```

**ì˜í–¥**: Planning, ì½”ë“œ ë“±ì´ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œë¨

#### 2.2. Add: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€

**íŒŒì¼**: `agentic-ai/cli/components/chat_panel.py`

**ì¶”ê°€ ë©”ì„œë“œ**:
```python
def add_streaming_message(self, role: str, content: str) -> str:
    """Start a streaming message (returns message_id)"""
    message_id = f"msg_{self.message_count}"
    # Create panel with initial content
    # Return ID for updating
    return message_id

def update_streaming_message(self, message_id: str, content: str) -> None:
    """Update streaming message with new content"""
    # Update the panel with accumulated content
    pass

def finalize_streaming_message(self, message_id: str) -> None:
    """Finalize streaming message"""
    # Mark as complete, apply final formatting
    pass
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# Start streaming
msg_id = chat.add_streaming_message("assistant", "")

# Update as tokens arrive
async for token in llm_stream():
    chat.update_streaming_message(msg_id, accumulated_text)

# Finalize
chat.finalize_streaming_message(msg_id)
```

**ì˜í–¥**: LLM ì‘ë‹µì´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ì„

#### 2.3. Add: ìƒíƒœ ë©”ì‹œì§€ í‘œì‹œ (ì„ì‹œ ë©”ì‹œì§€)

**íŒŒì¼**: `agentic-ai/cli/components/chat_panel.py`

**ì¶”ê°€ ë©”ì„œë“œ**:
```python
def add_status(self, message: str, style: str = "dim") -> str:
    """Add temporary status message

    Returns:
        status_id: ID for removing later
    """
    status_id = f"status_{int(time.time())}"

    text = Text()
    text.append("â— ", style="yellow")
    text.append(message, style=style)

    self.write(text)

    return status_id

def remove_status(self, status_id: str) -> None:
    """Remove temporary status message"""
    # Remove the status line
    pass
```

**ì‚¬ìš© ì˜ˆì‹œ**:
```python
# Show status
status_id = chat.add_status("ê³„íš ìˆ˜ë¦½ ì¤‘...")

# ... processing ...

# Remove when done
chat.remove_status(status_id)

# Show result
chat.add_message("assistant", "Here's the plan...")
```

**ì˜í–¥**: ì‚¬ìš©ìê°€ í˜„ì¬ ë¬´ìŠ¨ ì¼ì´ ì¼ì–´ë‚˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ

### Priority 3: â­ MEDIUM (í¸ì˜ì„± ê°œì„ )

#### 3.1. Add: ì½”ë“œ ë¸”ë¡ ìë™ ê°ì§€ ë° Syntax Highlighting

**íŒŒì¼**: `agentic-ai/cli/components/chat_panel.py`

**ë¡œì§**:
```python
def _detect_format(self, content: str) -> tuple[str, str]:
    """Detect content format and language

    Returns:
        (format, language):
            format: "text", "markdown", "code"
            language: "python", "typescript", "bash", etc.
    """
    # Check for code blocks
    if "```" in content:
        # Extract language from ```language
        match = re.match(r"```(\w+)", content)
        if match:
            return ("code", match.group(1))
        return ("code", "text")

    # Check for markdown indicators
    markdown_patterns = ["# ", "## ", "* ", "- ", "1. ", "[", "]("]
    if any(pattern in content for pattern in markdown_patterns):
        return ("markdown", None)

    return ("text", None)

def add_message_smart(self, role: str, content: str) -> None:
    """Add message with automatic format detection"""
    format_type, language = self._detect_format(content)

    if format_type == "markdown":
        self.add_message(role, content, format="markdown")
    elif format_type == "code":
        self.add_message(role, content, format="code", language=language)
    else:
        self.add_message(role, content, format="text")
```

**ì˜í–¥**: ìë™ìœ¼ë¡œ ì˜ˆì˜ê²Œ í‘œì‹œë¨

#### 3.2. Add: LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ Backend ìˆ˜ì •

**íŒŒì¼**: `agentic-ai/core/llm_client.py`

**í˜„ì¬**: ì „ì²´ ì‘ë‹µ ëŒ€ê¸° í›„ ë°˜í™˜

**ê°œì„ **:
```python
async def call_llm_stream(
    self,
    messages: List[Dict[str, str]],
    **kwargs
) -> AsyncIterator[str]:
    """Call LLM with streaming support

    Yields:
        str: Token chunks as they arrive
    """
    # Use OpenAI streaming API
    async with aiohttp.ClientSession() as session:
        async with session.post(
            f"{endpoint}/chat/completions",
            json={
                "messages": messages,
                "stream": True,  # âœ… Enable streaming
                **kwargs
            }
        ) as response:
            async for line in response.content:
                # Parse SSE format
                if line.startswith(b"data: "):
                    data = json.loads(line[6:])
                    if "choices" in data:
                        token = data["choices"][0]["delta"].get("content", "")
                        if token:
                            yield token
```

**ì‚¬ìš©**:
```python
# Workflowì—ì„œ
accumulated = ""
async for token in self.llm_client.call_llm_stream(messages):
    accumulated += token
    # Yield to UI
    yield {
        "type": "llm_streaming",
        "message": token,
        "accumulated": accumulated
    }
```

**ì˜í–¥**: LLM í† í°ì´ ì‹¤ì‹œê°„ìœ¼ë¡œ UIì— í‘œì‹œë¨

#### 3.3. Add: Execute nodeì—ì„œ ì•¡ì…˜ ìƒíƒœë¥¼ Chatì— í‘œì‹œ

**íŒŒì¼**: `agentic-ai/cli/backend_bridge.py`

**ì¶”ê°€ event type**:
```python
# Execute nodeì—ì„œ ë°œìƒì‹œí‚¬ ì´ë²¤íŠ¸
{
    "type": "action_start",
    "data": {
        "action": "READ_FILE",
        "file_path": "README.md"
    }
}

{
    "type": "action_complete",
    "data": {
        "action": "READ_FILE",
        "success": True,
        "result": "..."
    }
}
```

**UI ì²˜ë¦¬**:
```python
# app.py
elif update.type == "action_start":
    action = update.data.get("action")
    chat.add_status(f"ğŸ”§ Executing: {action}")

elif update.type == "action_complete":
    if update.data.get("success"):
        chat.add_status(f"âœ… {update.data['action']} completed")
    else:
        chat.add_status(f"âš ï¸ {update.data['action']} failed")
```

**ì˜í–¥**: ì‚¬ìš©ìê°€ ì–´ë–¤ ë„êµ¬ê°€ ì‹¤í–‰ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆìŒ

---

## ğŸ“‹ êµ¬í˜„ ìˆœì„œ (Implementation Order)

### Phase 1: Critical Fixes (1-2 hours)
1. âœ… Fix reflect_node overwriting should_continue
2. âœ… Add session ID generation and display
3. âœ… Add status messages to Chat panel

### Phase 2: Streaming Support (2-3 hours)
4. âœ… Add streaming methods to ChatPanel
5. âœ… Add LLM streaming support to llm_client
6. âœ… Wire up streaming from backend to UI

### Phase 3: Visual Improvements (1-2 hours)
7. âœ… Add Markdown rendering to ChatPanel
8. âœ… Add Syntax highlighting for code blocks
9. âœ… Add auto-detection of content format

### Phase 4: UX Polish (1 hour)
10. âœ… Add action execution feedback
11. âœ… Add temporary status messages
12. âœ… Test and refine

---

## ğŸ¯ Expected Results

### Before (Current)
```
User: "Create a plan for..."
[Long wait with no feedback]
[Progress bar updates in small area]
Assistant: [Sudden appearance of full result]

Status: No session
Error: Task failed: Max iterations reached
```

### After (Improved)
```
User: "Create a plan for..."

â— Processing your request...
â— ğŸ“‹ Planning task execution...
â— ğŸ¤” Analyzing complexity (0.6)...
â— ğŸ”§ Executing: LIST_DIRECTORY
  âœ… Found 15 files
â— ğŸ”§ Executing: READ_FILE (README.md)
  âœ… Read 250 lines
â— ğŸ’­ Reflecting on progress (2/5 steps completed)