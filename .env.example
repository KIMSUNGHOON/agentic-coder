# ============================================
# AI Code Assistance Application Configuration
# ============================================
# Copy this file to .env and modify as needed
# cp .env.example .env

# =========================
# LLM Configuration
# =========================

# Primary LLM endpoint (used as default for all tasks)
# IMPORTANT: Use localhost or 127.0.0.1, NOT 0.0.0.0 for client connections
LLM_ENDPOINT=http://localhost:8001/v1
LLM_MODEL=deepseek-ai/DeepSeek-R1

# Model type override (OPTIONAL - auto-detected from model name)
# The system automatically detects model type from model names:
#   - "deepseek" in name → deepseek prompts (<think> tags)
#   - "qwen" in name → qwen prompts
#   - "gpt-oss" in name → gpt-oss prompts (Harmony format, reasoning effort)
#   - "gpt/openai" in name → gpt prompts
#   - "claude/anthropic" in name → claude prompts
#   - others → generic prompts
# Only set this if you need to override auto-detection
# MODEL_TYPE=deepseek

# =========================
# Load Balancing Configuration (RECOMMENDED for multiple identical servers)
# =========================
# Use this when you have multiple vLLM servers running the SAME model
# Example: 2x GPT-OSS-120B servers for load balancing
# The system will automatically round-robin requests across all endpoints
# VLLM_ENDPOINTS=http://localhost:8001/v1,http://localhost:8002/v1

# =========================
# Task-Based Routing (Alternative - for different models per task)
# =========================
# Use this when you have DIFFERENT models for reasoning vs coding
# Task-specific endpoints (override LLM_ENDPOINT for specific tasks)
# VLLM_REASONING_ENDPOINT=http://localhost:8001/v1
# VLLM_CODING_ENDPOINT=http://localhost:8002/v1

# Task-specific models (auto-detected model type for each)
# Example: DeepSeek for reasoning, Qwen for coding
REASONING_MODEL=deepseek-ai/DeepSeek-R1
CODING_MODEL=Qwen/Qwen3-8B-Coder

# Task-specific model type overrides (OPTIONAL)
# Use when you need different adapters per task
# REASONING_MODEL_TYPE=gpt-oss
# CODING_MODEL_TYPE=gpt-oss

# =========================
# GPT-OSS Configuration (OpenAI's Open-Source Model)
# =========================
# For using GPT-OSS-120B for both reasoning and coding:
# VLLM_REASONING_ENDPOINT=http://localhost:8001/v1
# VLLM_CODING_ENDPOINT=http://localhost:8002/v1
# REASONING_MODEL=openai/gpt-oss-120b
# CODING_MODEL=openai/gpt-oss-120b
#
# Reasoning effort level: low (default, fast), medium (balanced), high (thorough)
GPT_OSS_REASONING_EFFORT=low

# =========================
# vLLM Server Startup Commands
# =========================
# CRITICAL: For GPT-OSS Tool Calling Support
#
# GPT-OSS-120B requires specific flags for function/tool calling:
# --tool-call-parser openai --enable-auto-tool-choice
#
# Example startup commands:
#
# 1. GPT-OSS-120B (Reasoning + Tool Calling):
#    vllm serve openai/gpt-oss-120b \
#      --host 0.0.0.0 --port 8001 \
#      --tool-call-parser openai \
#      --enable-auto-tool-choice \
#      --gpu-memory-utilization 0.90
#
# 2. DeepSeek-R1 (Reasoning with <think> tags):
#    vllm serve deepseek-ai/DeepSeek-R1 \
#      --host 0.0.0.0 --port 8001 \
#      --gpu-memory-utilization 0.90
#
# 3. Qwen3-8B-Coder (Coding):
#    vllm serve Qwen/Qwen3-8B-Coder \
#      --host 0.0.0.0 --port 8002 \
#      --gpu-memory-utilization 0.90
#
# IMPORTANT Notes:
# - GPT-OSS: Requires --tool-call-parser openai for function calling
# - GPT-OSS: Does NOT support parallel tool calling (calls 1 tool at a time)
# - GPT-OSS: Needs CoT from previous tool calls in message history
# - Use localhost or 127.0.0.1 for client connections (not 0.0.0.0)

# =========================
# Agent Framework Selection
# =========================
# Options: microsoft, langchain, deepagent
# - microsoft: Original Microsoft Agent Framework (default)
# - langchain: LangChain/LangGraph based agents
# - deepagent: DeepAgents with advanced capabilities
AGENT_FRAMEWORK=microsoft

# =========================
# Workflow Configuration
# =========================
# Maximum code review/refinement iterations
# Lower values = faster execution, higher values = better quality
# Recommended:
#   1 = Fast (no refinement, use first generated code)
#   2 = Balanced (1 review + 1 fix if needed)
#   3 = Quality (up to 3 review/fix cycles)
# Note: Each iteration adds ~30-60 seconds
MAX_REVIEW_ITERATIONS=1

# =========================
# API Server Configuration
# =========================
# Note: 0.0.0.0 is correct here for SERVER binding (accepts all interfaces)
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# =========================
# CORS Configuration
# =========================
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# =========================
# Workspace Configuration
# =========================
# Default workspace directory where all projects will be stored
# Structure: DEFAULT_WORKSPACE/{session_id}/{project_name}
# Example: /home/username/Workspaces/TestCode/session-abc123/calculator
#
# If not set, defaults to: {user_home}/workspace
# Linux/Mac: /home/username/workspace
# Windows: C:\Users\username\workspace
DEFAULT_WORKSPACE=/home/username/Workspaces/TestCode

# =========================
# Logging
# =========================
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# =========================
# Network Mode Configuration (Phase 2)
# =========================
# Controls which tools are available based on network access
# Options:
#   - online  : All tools available (default, requires internet for web search)
#   - offline : Only local tools (no external network requests)
#
# Security Policy:
#   - EXTERNAL_API tools (Tavily search, REST APIs) blocked in offline mode
#   - EXTERNAL_DOWNLOAD tools (wget/curl) allowed in offline mode
#   - LOCAL tools (file, code, git) always available
#
# Use 'offline' in secure/air-gapped networks
NETWORK_MODE=online

# =========================
# Agent Tools Configuration
# =========================
# Tavily API Key for Web Search Tool (ONLINE MODE ONLY)
# Get your API key at: https://tavily.com
# Leave empty to disable web search functionality
# Note: Ignored in offline mode
TAVILY_API_KEY=

# ChromaDB Path for Code Search Tool (WORKS OFFLINE)
# Default: ./chroma_db (relative to project root)
CHROMA_DB_PATH=./chroma_db

# =========================
# Sandbox Configuration (Phase 4)
# =========================
# AIO Sandbox for isolated code execution
# Docker container-based secure execution environment
#
# Quick Start (Local Server):
#   1. Pull image once: docker pull ghcr.io/agent-infra/sandbox:latest
#   2. Image is cached locally - works offline after first pull
#
# Docker image for sandbox
SANDBOX_IMAGE=ghcr.io/agent-infra/sandbox:latest

# Sandbox API settings (container exposes API on this port)
SANDBOX_HOST=localhost
SANDBOX_PORT=8080

# Execution limits
SANDBOX_TIMEOUT=60
SANDBOX_MEMORY=1g
SANDBOX_CPU=2.0

# =========================
# Sandbox Registry (Optional - Enterprise Only)
# =========================
# Only needed when deploying to multiple servers with central registry
# For single local server, leave this commented out
#
# Example for Harbor registry:
# SANDBOX_REGISTRY=harbor.company.com
#
# When set, image path becomes: {SANDBOX_REGISTRY}/sandbox/aio:latest
# SANDBOX_REGISTRY=
