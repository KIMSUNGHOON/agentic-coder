# ============================================
# AI Code Assistance Application Configuration
# ============================================
# Copy this file to .env and modify as needed
# cp .env.example .env

# =========================
# LLM Configuration
# =========================

# Primary LLM endpoint (used as default for all tasks)
# IMPORTANT: Use localhost or 127.0.0.1, NOT 0.0.0.0 for client connections
LLM_ENDPOINT=http://localhost:8001/v1
LLM_MODEL=deepseek-ai/DeepSeek-R1

# Model type override (OPTIONAL - auto-detected from model name)
# The system automatically detects model type from model names:
#   - "deepseek" in name → deepseek prompts (<think> tags)
#   - "qwen" in name → qwen prompts
#   - "gpt-oss" in name → gpt-oss prompts (Harmony format, reasoning effort)
#   - "gpt/openai" in name → gpt prompts
#   - "claude/anthropic" in name → claude prompts
#   - others → generic prompts
# Only set this if you need to override auto-detection
# MODEL_TYPE=deepseek

# Task-specific endpoints (override LLM_ENDPOINT for specific tasks)
VLLM_REASONING_ENDPOINT=http://localhost:8001/v1
VLLM_CODING_ENDPOINT=http://localhost:8002/v1

# Task-specific models (auto-detected model type for each)
# Example: DeepSeek for reasoning, Qwen for coding
REASONING_MODEL=deepseek-ai/DeepSeek-R1
CODING_MODEL=Qwen/Qwen3-8B-Coder

# Task-specific model type overrides (OPTIONAL)
# Use when you need different adapters per task
# REASONING_MODEL_TYPE=gpt-oss
# CODING_MODEL_TYPE=gpt-oss

# =========================
# GPT-OSS Configuration (OpenAI's Open-Source Model)
# =========================
# For using GPT-OSS-120B for both reasoning and coding:
# VLLM_REASONING_ENDPOINT=http://localhost:8001/v1
# VLLM_CODING_ENDPOINT=http://localhost:8002/v1
# REASONING_MODEL=openai/gpt-oss-120b
# CODING_MODEL=openai/gpt-oss-120b
#
# Reasoning effort level: low (default, fast), medium (balanced), high (thorough)
GPT_OSS_REASONING_EFFORT=low

# =========================
# Agent Framework Selection
# =========================
# Options: microsoft, langchain, deepagent
# - microsoft: Original Microsoft Agent Framework (default)
# - langchain: LangChain/LangGraph based agents
# - deepagent: DeepAgents with advanced capabilities
AGENT_FRAMEWORK=microsoft

# =========================
# API Server Configuration
# =========================
# Note: 0.0.0.0 is correct here for SERVER binding (accepts all interfaces)
API_HOST=0.0.0.0
API_PORT=8000
API_RELOAD=true

# =========================
# CORS Configuration
# =========================
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# =========================
# Workspace Configuration
# =========================
# Default workspace directory where all projects will be stored
# Structure: DEFAULT_WORKSPACE/{session_id}/{project_name}
# Example: /home/username/Workspaces/TestCode/session-abc123/calculator
#
# If not set, defaults to: {user_home}/workspace
# Linux/Mac: /home/username/workspace
# Windows: C:\Users\username\workspace
DEFAULT_WORKSPACE=/home/username/Workspaces/TestCode

# =========================
# Logging
# =========================
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO
